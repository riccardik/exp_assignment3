<html><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8">
<title>exp_assignment3: Main Page</title>
<link href="doxygen.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="jquery.js"></script>
</head>
<body onload='searchBox.OnSelectItem(0);'>
<!-- Generated by Doxygen 1.8.11 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">exp_assignment3 Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><div class="manifest" style="padding: 5px; background-color: #eee; border: 1px solid #333; float: right; width: 25%;">
<h3>exp_assignment3</h3>
<p class=description><em>The exp_assignment3 package</em></p>
<p>
<ul>
<li>Homepage: <a href="http://wiki.ros.org/exp_assignment3">http://wiki.ros.org/exp_assignment3</a></li>
</ul>
</p>

</div>
<p><b>Package</b> for the assignment 3 of Esperimental Robotics Laboratory</p>
<h1>Experimental Robotics Laboratory - Assignment 3</h1>
<h2>Riccardo Lastrico - 4070551</h2>
<h3>Assignment contents</h3>
<p>The assignment content is an exercise based on ROS about a finite state machine that represent a pet-like robot behavior; this robot moves in a 3D simulation using Gazebo and is equipped with an RGB camera. Using that camera, is able to recognize different balls of different colors; each ball is associated to a different room and when it is first seen its position is associated to the correspondent room. The robot can navigate around the ambient and find the ball and, when requested, is able to go back to a precise room that has been previously discovered.</p>
<h4>Map of the environment</h4>
<p>The ambient is devided in rooms, each room has inside a ball of a particular color:</p>
<ul>
<li>Entrance, blue;</li>
<li>Closet, red;</li>
<li>Living room, green;</li>
<li>Kitchen, yellow;</li>
<li>Bathroom, magenta;</li>
<li>Bedroom, black.</li>
</ul>
<div class="image">
<img src="./map.png"  alt="map"/>
</div>
 <h3>System architecture</h3>
<p>This is a graph of the system architecture (obtained via rqt-graph):</p>
<div class="image">
<img src="./system_architecture.png"  alt="System Architecture"/>
</div>
<h5>Implemented nodes</h5>
<p>The node <code>state_miro</code> contains the state machine that will be described right after this paragraph.</p>
<p>The node <code>cmd_generator</code> sends commands to the state machine; a successful "PLAY" command will make the robot go to the human position and then a location command will succeed; the robot wait for some time for the instruction to which location to go, if that is not received it will go back wandering in the map.</p>
<p>The node <code>image_feature</code> receives the image from the camera and if detects a ball sends a message to the <code>state_miro</code> node; when the state machine will change state, this node will also send <code>cmd_vel</code> messages to the robot depending on the distance from the ball.</p>
<p><code>move_base</code>, this node will allow to navigate in the map to a given xy coordinate, using a global and a local planner for the navigation.</p>
<p><code>explore_lite</code>, this node allows to map the ambient, navigation towards unexplored frontier of the visible map.</p>
<p>The node <code>gmapping</code> takes informations about the physical obstacles in front of the robot using the Lidar sensor; those data will be used to build the map of the ambient.</p>
<p>Snapshot of the map while it is being built(obtaned via rviz):</p>
<div class="image">
<img src="./map_building.png"  alt="System Architecture"/>
</div>
<h3>System states</h3>
<p>This is a graph of the possible states (obtained via smach-viewer):</p>
<div class="image">
<img src="./state_machine.png"  alt="System Architecture"/>
</div>
<p>The possible states are: </p><ul>
<li>
<code>SLEEP</code>, the robot is sleeping and so it wont respond to any command. After a while in the <code>NORMAL</code>, <code>FIND</code> or <code>PLAY</code> state, even if not commanded to do so, it reaches location []. After some amount of time, the robot goes again in state <code>NORMAL</code>.  </li>
<li>
<p class="startli"><code>NORMAL</code>, the robot is in the predefined state, it moves around the map until he detects a ball in his lane of sight: the robot will than pass to the <code>TRACK</code> state. This behavior is driven by the <code>explore_lite</code> package until the complete map has been reconstructed and all the ball has been found, then some target coordinates are radomly generated. If the robot receives a <code>goPlay command</code> it will switch to the <code>PLAY</code> state.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><code>PLAY</code>, the robot enters in this state from the <code>NORMAL</code> one receiving a command. It starts to approach the human, unless it receive a location command (to avoid loosing to much time in waiting the robot to reach him), and when it has gone near him it awaits for a location command. If the location's position is known than the robot will reach that position, otherwhise it will go to the <code>FIND</code> state. When the location is found or reach it will go back to this state, the robot will approach again the human and the behavior resets. If a location command is not given, after some time the robot will reset to the <code>NORMAL</code> state.</p>
<p></p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><code>TRACK</code>, the robot enters this state from the <code>NORMAL </code> or the <code>FIND</code> state. It stops the exploration performed with the <code>explore_lite</code> package and it approaches the ball, saving its location. After that, it will go back to the previous state.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><code>FIND</code>, the robot enters this state from <code>PLAY</code>, it abilitates the <code>explore_lite</code> driven exploration and it starts to find the ball related to a precise location. The behavior is very similar to the <code>NORMAL</code> state, the only difference is that this state stops when the particular ball has been found, going back to the <code>PLAY</code> state, (if he finds other colors they will be saved but the search process won't be interrupted) or if it goes in timeout, than it will go back to the <code>NORMAL</code> state.</p>
<p></p>
<p class="endli"></p>
</li>
</ul>
<h5>Packages and file list</h5>
<p>The package containing the assignment is <code>exp_assignment3</code>, in addition we have <code>gmapping</code> and <code>explore_lite</code>. In particular, we have:</p>
<ul>
<li><code>scripts</code> folder:<ul>
<li><code><a class="el" href="robot__following3_8py.html">robot_following3.py</a></code>: contains the code <code>image_feature</code>, this node perform the blob detection on the image from the camera and if the robot is in the <code>TRACK</code> state also sends <code>cmd_vel</code> commands to the robot to approach a ball in front of it; it also advises the state machine if a ball with unknown position has been detected.<ul>
<li><code><a class="el" href="state__machine__miro__ext_8py.html">state_machine_miro_ext.py</a></code>: the smach state machine of the robot;</li>
<li><code><a class="el" href="state__c_8py.html">state_c.py</a></code>: gui application to send commands to the robot;</li>
<li><code><a class="el" href="state__c__random_8py.html">state_c_random.py</a></code>: node that radomly sends commands to the robot;</li>
</ul>
</li>
</ul>
</li>
<li><code>launch</code> folder:<ul>
<li><code>display.launch</code>: launch file that launches Rviz to check the model of the robot;</li>
<li><code>sim_controlled.launch</code>: launches the simulation where commands are received from the user;</li>
<li><code>sim_random.launch</code>: launches the simulation where commands are generated randomly;<ul>
<li><code>gmapping.launch</code>: launches the mapping procedure;</li>
<li><code>move_base.launch</code>: launches the node that allows the trajectory planning;</li>
</ul>
</li>
<li><code>simulation.launches</code>: launches the simulation, it is not to be called directly.</li>
</ul>
</li>
</ul>
<h3>Installation</h3>
<p>This is a ROS package, so it will be necessary to clone this repository into the <code>src</code> folder of a ROS workspace (here is assumed to be named <code>my_ros</code>):</p>
<pre class="fragment">cd ~/my_ros/src
git clone
catkin_make --pkg explore_lite
</pre><p>Some packages are needed: </p><pre class="fragment">smach-viewer
cv_bridge
image_transport
move_base
    explore_lite (modified version, provided)
    gmapping (provided)
</pre><h3>Run the simulation</h3>
<p>To easily run the simulation i created a launch file, that can be used this way:</p>
<pre class="fragment">source ~/my_ros/devel/setup.bash 
roslaunch exp_assignment3 simulation_random.launch
</pre><p>This runs the random simulation, the state of the robot will be outputted on the shell with the information about the command received and an eventual change of state; unfortunately, due to the implementation of the packages <code>explore_lite</code> and <code>move_base</code> the output is very cluttered.</p>
<p>If you want to interact with the simulation by giving direct commands you have to run: </p><pre class="fragment">source ~/my_ros/devel/setup.bash 
roslaunch exp_assignment3 simulation_controlled.launch
</pre><p>and then open a new shell and run: </p><pre class="fragment">source ~/my_ros/devel/setup.bash 
rosrun exp_assignment3 state_c.py
</pre><p>A command line interface allows to send commands to make the robot go in the <code>PLAY</code> state and to receive commands about which location to reach.</p>
<p>For both, an additional window will open: this contains the output image from the robot's camera:</p>
<div class="image">
<img src="./camera.png"  alt="cameraview"/>
</div>
 <h3>System features</h3>
<p>The system features a finite-state machine using the <code>Smach</code> packet and a simulation done via Gazebo. The robot, using "color blob recognition", is able to recognize a ball in the image provied by the camera; the robot itself features the camera, two actuated joints (the wheels) and an additional one, the neck, that can be moved around the roll axis. The system also features the possibility to be controlled by the user or behave fully randomly, by writing the coordinates in the appropriate topic. The system have mapping features: using a LIDAR sensor, is able to map the ambient and then be able to navigate into it. The mapping is done in conjunction with the <code>explore_lite</code> package, that contains an algorithm that devides the areas to be mapped into frontiers and tries to optimize the mapping process. The mapping in according with the colored-ball recognition feature, will allow to map the ambient and collect information about the actual position of the room, associating it with the correspondent ball. After the mapping process, the robot is then able to navigate and, when the ball are recognized, to be able to reach a particular room if asked so.</p>
<p>The robot features an RGB camera and a Hokuyo Lidar sensor; i set the same field of view (90°) for both devices, this allows me to be sure that the mapping and the ball find process progresses together. In addition, the robot has an unactuated caster wheel (just for stability) and two motorized fixed wheels, controlled via a differential wheel drive control system. This is the robot:</p>
<div class="image">
<img src="./robot_model.png"  alt="robot"/>
</div>
<h3>System limitation</h3>
<p>Sometimes the <code>FIND</code> behavior is a bit slow to start, due to some momentaneous conflict with <code>explore_lite</code> and <code>move_base</code> and so is not able to find anything because it goes in timeout; this behavior is a bit random so i wasn't really able to understand why. Depending on <code>explore_lite</code> and starting/stopping it when it necessary it looks like some time that node needs some time to generate a new goal location.</p>
<h3>Possible improvements</h3>
<p>The overall system requires a very high cpu load, so the simulation do not run at run time (being in a VM also do not help), maybe some optimization could be made, for instance reducing the image resolution or lowering the framerate. In my case i had to put the Lidar sensor a bit far off the robot model because otherwhise it wouldn't be able to properly detect the obstacles but detecting himself, a bit more study of the positioning could make it more elegant. </p>
<h3>Documentation</h3>
<p>The documentation is accessible in: </p><pre class="fragment">./doc/html/index.html
</pre><h3>Contacts</h3>
<p>Riccardo Lastrico - 4070551</p>
<p>Email: <a href="#" onclick="location.href='mai'+'lto:'+'rik'+'y.'+'las'+'tr'+'ico'+'@g'+'mai'+'l.'+'com'; return false;">riky.<span style="display: none;">.nosp@m.</span>last<span style="display: none;">.nosp@m.</span>rico@<span style="display: none;">.nosp@m.</span>gmai<span style="display: none;">.nosp@m.</span>l.com</a> </p>
</div></div><!-- contents -->

<br clear="all" />
<hr size="1"><div style="align: right;">
<a href="http://wiki.ros.org/exp_assignment3">exp_assignment3</a><br />
Author(s): </br />
<small>autogenerated on Wed Feb 24 2021 19:47:33</small>
</div>
</body>
</html>
